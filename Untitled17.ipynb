{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LJrYv7cyAMD",
        "outputId": "e18b7ccb-9283-4e36-e06e-59a8815223ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Data Loading and Preprocessing\n",
        "\n",
        "# Install necessary libraries if not already present (Colab usually has them)\n",
        "# !pip install tensorflow numpy pandas\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "import os # To check if file exists\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load the Text ---\n",
        "# >>> IMPORTANT: Replace 'your_text_file.txt' with the exact name of your uploaded file. <<<\n",
        "file_name = 'your_text_file.txt' # e.g., 'tinyshakespeare.txt', 'alice.txt'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Error: The file '{file_name}' was not found in the Colab session storage.\")\n",
        "    print(\"Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\")\n",
        "    # You might want to stop execution here if the file is critical\n",
        "    # raise FileNotFoundError(f\"File not found: {file_name}\")\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "    print(f\"Successfully loaded text from '{file_name}'. Length: {len(raw_text)} characters.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # Consider exiting or handling the error if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 2. Clean It Up ---\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = raw_text.lower() # Convert all text to lowercase\n",
        "\n",
        "# Remove anything that's not a letter, space, or basic punctuation.\n",
        "# Keeping some punctuation can help with coherence for better models.\n",
        "# For simplicity, let's allow letters, spaces, and basic sentence-ending punctuation.\n",
        "cleaned_text = re.sub(r'[^a-z\\s.,!?]', '', cleaned_text)\n",
        "# Replace multiple spaces with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Cleaned text (first 500 chars): {cleaned_text[:500]}...\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n",
        "\n",
        "\n",
        "# --- 3. Turn Words into Numbers (Tokenization) ---\n",
        "print(\"\\nTokenizing words...\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text]) # Let the tokenizer learn all unique words\n",
        "\n",
        "word_to_id = tokenizer.word_index # Your word-to-number mapping\n",
        "id_to_word = {v: k for k, v in word_to_id.items()} # Your number-to-word mapping\n",
        "total_words = len(word_to_id) + 1 # Total unique words (+1 for potential OOV or padding)\n",
        "\n",
        "# Convert your entire text into a sequence of numbers\n",
        "numbered_text = tokenizer.texts_sequences([cleaned_text])[0]\n",
        "print(f\"Total unique words (vocabulary size): {total_words - 1}\") # -1 because of the +1 for padding token\n",
        "print(f\"First 20 words as numbers: {numbered_text[:20]}\")\n",
        "\n",
        "\n",
        "# --- 4. Create \"Training Pairs\" (Sequences) ---\n",
        "print(\"\\nCreating training sequences...\")\n",
        "sequence_length = 50 # How many words the model \"sees\" at once to predict the next\n",
        "input_sequences = []\n",
        "\n",
        "# Ensure we have enough text to form at least one sequence\n",
        "if len(numbered_text) < sequence_length + 1:\n",
        "    print(f\"Error: Not enough text to create sequences. Minimum length needed: {sequence_length + 1} tokens.\")\n",
        "    print(f\"Current text length (in tokens): {len(numbered_text)}\")\n",
        "    print(\"Please use a larger text file or reduce 'sequence_length'.\")\n",
        "    # Exit or raise error if data is insufficient for training\n",
        "    exit()\n",
        "\n",
        "for i in range(len(numbered_text) - sequence_length):\n",
        "    # Input: sequence_length words, Output: the (sequence_length + 1)th word\n",
        "    input_seq = numbered_text[i : i + sequence_length]\n",
        "    output_word = numbered_text[i + sequence_length]\n",
        "    input_sequences.append(input_seq + [output_word]) # Store as [word1, ..., word_N, next_word]\n",
        "\n",
        "print(f\"Number of raw sequences created: {len(input_sequences)}\")\n",
        "\n",
        "# Make all sequences the same length (add zeros if needed at the beginning)\n",
        "# The maxlen will be sequence_length + 1 (input sequence + 1 target word)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate into X (input features) and y (what we want to predict)\n",
        "X = padded_sequences[:, :-1] # All columns except the last one (the input sequence)\n",
        "y = padded_sequences[:, -1]  # Only the last column (the word to predict)\n",
        "\n",
        "# Convert 'y' to a special format (one-hot encoding) for the model's output layer\n",
        "# This means for each word, it's represented as a vector of zeros with a '1' at its corresponding index\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"Shape of X (inputs): {X.shape} (Number of sequences, Sequence length)\")\n",
        "print(f\"Shape of y (outputs): {y.shape} (Number of sequences, Vocabulary size)\")\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# You're now ready to define and train the LSTM model in the next cells."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "7WjFM6kJzJXi",
        "outputId": "3a459541-2973-4fcb-a7d4-2e83b0f3831c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Error: The file 'your_text_file.txt' was not found in the Colab session storage.\n",
            "Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\n",
            "An error occurred while reading the file: [Errno 2] No such file or directory: 'your_text_file.txt'\n",
            "\n",
            "Cleaning text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'raw_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1542584880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# --- 2. Clean It Up ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCleaning text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert all text to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Remove anything that's not a letter, space, or basic punctuation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0OMm3izfpm",
        "outputId": "24697cc1-91bb-4e4f-dad0-58e95d3639e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Data Loading and Preprocessing\n",
        "\n",
        "# Install necessary libraries if not already present (Colab usually has them)\n",
        "# !pip install tensorflow numpy pandas\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "import os # To check if file exists\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load the Text ---\n",
        "# >>> IMPORTANT: Replace 'your_text_file.txt' with the exact name of your uploaded file. <<<\n",
        "file_name = 'your_text_file.txt' # e.g., 'tinyshakespeare.txt', 'alice.txt'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Error: The file '{file_name}' was not found in the Colab session storage.\")\n",
        "    print(\"Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\")\n",
        "    # You might want to stop execution here if the file is critical\n",
        "    # raise FileNotFoundError(f\"File not found: {file_name}\")\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "    print(f\"Successfully loaded text from '{file_name}'. Length: {len(raw_text)} characters.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # Consider exiting or handling the error if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 2. Clean It Up ---\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = raw_text.lower() # Convert all text to lowercase\n",
        "\n",
        "# Remove anything that's not a letter, space, or basic punctuation.\n",
        "# Keeping some punctuation can help with coherence for better models.\n",
        "# For simplicity, let's allow letters, spaces, and basic sentence-ending punctuation.\n",
        "cleaned_text = re.sub(r'[^a-z\\s.,!?]', '', cleaned_text)\n",
        "# Replace multiple spaces with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Cleaned text (first 500 chars): {cleaned_text[:500]}...\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n",
        "\n",
        "\n",
        "# --- 3. Turn Words into Numbers (Tokenization) ---\n",
        "print(\"\\nTokenizing words...\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text]) # Let the tokenizer learn all unique words\n",
        "\n",
        "word_to_id = tokenizer.word_index # Your word-to-number mapping\n",
        "id_to_word = {v: k for k, v in word_to_id.items()} # Your number-to-word mapping\n",
        "total_words = len(word_to_id) + 1 # Total unique words (+1 for potential OOV or padding)\n",
        "\n",
        "# Convert your entire text into a sequence of numbers\n",
        "numbered_text = tokenizer.texts_sequences([cleaned_text])[0]\n",
        "print(f\"Total unique words (vocabulary size): {total_words - 1}\") # -1 because of the +1 for padding token\n",
        "print(f\"First 20 words as numbers: {numbered_text[:20]}\")\n",
        "\n",
        "\n",
        "# --- 4. Create \"Training Pairs\" (Sequences) ---\n",
        "print(\"\\nCreating training sequences...\")\n",
        "sequence_length = 50 # How many words the model \"sees\" at once to predict the next\n",
        "input_sequences = []\n",
        "\n",
        "# Ensure we have enough text to form at least one sequence\n",
        "if len(numbered_text) < sequence_length + 1:\n",
        "    print(f\"Error: Not enough text to create sequences. Minimum length needed: {sequence_length + 1} tokens.\")\n",
        "    print(f\"Current text length (in tokens): {len(numbered_text)}\")\n",
        "    print(\"Please use a larger text file or reduce 'sequence_length'.\")\n",
        "    # Exit or raise error if data is insufficient for training\n",
        "    exit()\n",
        "\n",
        "for i in range(len(numbered_text) - sequence_length):\n",
        "    # Input: sequence_length words, Output: the (sequence_length + 1)th word\n",
        "    input_seq = numbered_text[i : i + sequence_length]\n",
        "    output_word = numbered_text[i + sequence_length]\n",
        "    input_sequences.append(input_seq + [output_word]) # Store as [word1, ..., word_N, next_word]\n",
        "\n",
        "print(f\"Number of raw sequences created: {len(input_sequences)}\")\n",
        "\n",
        "# Make all sequences the same length (add zeros if needed at the beginning)\n",
        "# The maxlen will be sequence_length + 1 (input sequence + 1 target word)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate into X (input features) and y (what we want to predict)\n",
        "X = padded_sequences[:, :-1] # All columns except the last one (the input sequence)\n",
        "y = padded_sequences[:, -1]  # Only the last column (the word to predict)\n",
        "\n",
        "# Convert 'y' to a special format (one-hot encoding) for the model's output layer\n",
        "# This means for each word, it's represented as a vector of zeros with a '1' at its corresponding index\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"Shape of X (inputs): {X.shape} (Number of sequences, Sequence length)\")\n",
        "print(f\"Shape of y (outputs): {y.shape} (Number of sequences, Vocabulary size)\")\n",
        "print(\"Data preprocessing complete!\")\n",
        "# ... (rest of Colab Cell 1 code) ...\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# --- ADD THESE TWO LINES FOR DEBUGGING ---\n",
        "print(f\"DEBUG: X.shape before model definition: {X.shape}\")\n",
        "print(f\"DEBUG: X.dtype before model definition: {X.dtype}\")\n",
        "# --- END DEBUG LINES ---\n",
        "\n",
        "# You're now ready to define and train the LSTM model in the next cells."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "NZLZerRF0VZ3",
        "outputId": "4f8d396c-a11e-4bc5-e86c-b465758a5881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Error: The file 'your_text_file.txt' was not found in the Colab session storage.\n",
            "Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\n",
            "An error occurred while reading the file: [Errno 2] No such file or directory: 'your_text_file.txt'\n",
            "\n",
            "Cleaning text...\n",
            "Cleaned text (first 500 chars): first citizen before we proceed any further, hear me speak. all speak, speak. first citizen you are all resolved rather to die than to famish? all resolved. resolved. first citizen first, you know caius marcius is chief enemy to the people. all we knowt, we knowt. first citizen let us kill him, and well have corn at our own price. ist a verdict? all no more talking ont let it be done away, away! second citizen one word, good citizens. first citizen we are accounted poor citizens, the patricians ...\n",
            "Cleaned text length: 5175 characters.\n",
            "\n",
            "Tokenizing words...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tokenizer' object has no attribute 'texts_sequences'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2973079532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Convert your entire text into a sequence of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mnumbered_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total unique words (vocabulary size): {total_words - 1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -1 because of the +1 for padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First 20 words as numbers: {numbered_text[:20]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'texts_sequences'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'tinyshakespeare.txt'"
      ],
      "metadata": {
        "id": "CBG3AM340s3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Data Loading and Preprocessing\n",
        "\n",
        "# Install necessary libraries if not already present (Colab usually has them)\n",
        "# !pip install tensorflow numpy pandas\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "import os # To check if file exists\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load the Text ---\n",
        "# >>> IMPORTANT: Replace 'your_text_file.txt' with the exact name of your uploaded file. <<<\n",
        "file_name = 'your_text_file.txt' # e.g., 'tinyshakespeare.txt', 'alice.txt'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Error: The file '{file_name}' was not found in the Colab session storage.\")\n",
        "    print(\"Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\")\n",
        "    # You might want to stop execution here if the file is critical\n",
        "    # raise FileNotFoundError(f\"File not found: {file_name}\")\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "    print(f\"Successfully loaded text from '{file_name}'. Length: {len(raw_text)} characters.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # Consider exiting or handling the error if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 2. Clean It Up ---\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = raw_text.lower() # Convert all text to lowercase\n",
        "\n",
        "# Remove anything that's not a letter, space, or basic punctuation.\n",
        "# Keeping some punctuation can help with coherence for better models.\n",
        "# For simplicity, let's allow letters, spaces, and basic sentence-ending punctuation.\n",
        "cleaned_text = re.sub(r'[^a-z\\s.,!?]', '', cleaned_text)\n",
        "# Replace multiple spaces with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Cleaned text (first 500 chars): {cleaned_text[:500]}...\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n",
        "\n",
        "\n",
        "# --- 3. Turn Words into Numbers (Tokenization) ---\n",
        "print(\"\\nTokenizing words...\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text]) # Let the tokenizer learn all unique words\n",
        "\n",
        "word_to_id = tokenizer.word_index # Your word-to-number mapping\n",
        "id_to_word = {v: k for k, v in word_to_id.items()} # Your number-to-word mapping\n",
        "total_words = len(word_to_id) + 1 # Total unique words (+1 for potential OOV or padding)\n",
        "\n",
        "# Convert your entire text into a sequence of numbers\n",
        "numbered_text = tokenizer.texts_sequences([cleaned_text])[0]\n",
        "print(f\"Total unique words (vocabulary size): {total_words - 1}\") # -1 because of the +1 for padding token\n",
        "print(f\"First 20 words as numbers: {numbered_text[:20]}\")\n",
        "\n",
        "\n",
        "# --- 4. Create \"Training Pairs\" (Sequences) ---\n",
        "print(\"\\nCreating training sequences...\")\n",
        "sequence_length = 50 # How many words the model \"sees\" at once to predict the next\n",
        "input_sequences = []\n",
        "\n",
        "# Ensure we have enough text to form at least one sequence\n",
        "if len(numbered_text) < sequence_length + 1:\n",
        "    print(f\"Error: Not enough text to create sequences. Minimum length needed: {sequence_length + 1} tokens.\")\n",
        "    print(f\"Current text length (in tokens): {len(numbered_text)}\")\n",
        "    print(\"Please use a larger text file or reduce 'sequence_length'.\")\n",
        "    # Exit or raise error if data is insufficient for training\n",
        "    exit()\n",
        "\n",
        "for i in range(len(numbered_text) - sequence_length):\n",
        "    # Input: sequence_length words, Output: the (sequence_length + 1)th word\n",
        "    input_seq = numbered_text[i : i + sequence_length]\n",
        "    output_word = numbered_text[i + sequence_length]\n",
        "    input_sequences.append(input_seq + [output_word]) # Store as [word1, ..., word_N, next_word]\n",
        "\n",
        "print(f\"Number of raw sequences created: {len(input_sequences)}\")\n",
        "\n",
        "# Make all sequences the same length (add zeros if needed at the beginning)\n",
        "# The maxlen will be sequence_length + 1 (input sequence + 1 target word)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate into X (input features) and y (what we want to predict)\n",
        "X = padded_sequences[:, :-1] # All columns except the last one (the input sequence)\n",
        "y = padded_sequences[:, -1]  # Only the last column (the word to predict)\n",
        "\n",
        "# Convert 'y' to a special format (one-hot encoding) for the model's output layer\n",
        "# This means for each word, it's represented as a vector of zeros with a '1' at its corresponding index\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"Shape of X (inputs): {X.shape} (Number of sequences, Sequence length)\")\n",
        "print(f\"Shape of y (outputs): {y.shape} (Number of sequences, Vocabulary size)\")\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# You're now ready to define and train the LSTM model in the next cells."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "47eY9zGk1BUD",
        "outputId": "7b8eb72d-91f6-4e1d-ed7e-d573db7e845c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Error: The file 'your_text_file.txt' was not found in the Colab session storage.\n",
            "Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\n",
            "An error occurred while reading the file: [Errno 2] No such file or directory: 'your_text_file.txt'\n",
            "\n",
            "Cleaning text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'raw_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1542584880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# --- 2. Clean It Up ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCleaning text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert all text to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Remove anything that's not a letter, space, or basic punctuation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Data Loading and Preprocessing\n",
        "\n",
        "# Install necessary libraries if not already present (Colab usually has them)\n",
        "# !pip install tensorflow numpy pandas\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "import os # To check if file exists\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load the Text ---\n",
        "# >>> IMPORTANT: Replace 'your_text_file.txt' with the exact name of your uploaded file. <<<\n",
        "file_name = 'tinyshakespeare.txt' # e.g., 'tinyshakespeare.txt', 'alice.txt'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Error: The file '{file_name}' was not found in the Colab session storage.\")\n",
        "    print(\"Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\")\n",
        "    # You might want to stop execution here if the file is critical\n",
        "    # raise FileNotFoundError(f\"File not found: {file_name}\")\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "    print(f\"Successfully loaded text from '{file_name}'. Length: {len(raw_text)} characters.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # Consider exiting or handling the error if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 2. Clean It Up ---\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = raw_text.lower() # Convert all text to lowercase\n",
        "\n",
        "# Remove anything that's not a letter, space, or basic punctuation.\n",
        "# Keeping some punctuation can help with coherence for better models.\n",
        "# For simplicity, let's allow letters, spaces, and basic sentence-ending punctuation.\n",
        "cleaned_text = re.sub(r'[^a-z\\s.,!?]', '', cleaned_text)\n",
        "# Replace multiple spaces with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Cleaned text (first 500 chars): {cleaned_text[:500]}...\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n",
        "\n",
        "\n",
        "# --- 3. Turn Words into Numbers (Tokenization) ---\n",
        "print(\"\\nTokenizing words...\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text]) # Let the tokenizer learn all unique words\n",
        "\n",
        "word_to_id = tokenizer.word_index # Your word-to-number mapping\n",
        "id_to_word = {v: k for k, v in word_to_id.items()} # Your number-to-word mapping\n",
        "total_words = len(word_to_id) + 1 # Total unique words (+1 for potential OOV or padding)\n",
        "\n",
        "# Convert your entire text into a sequence of numbers\n",
        "numbered_text = tokenizer.texts_to_sequences([cleaned_text])[0]\n",
        "print(f\"Total unique words (vocabulary size): {total_words - 1}\") # -1 because of the +1 for padding token\n",
        "print(f\"First 20 words as numbers: {numbered_text[:20]}\")\n",
        "\n",
        "\n",
        "# --- 4. Create \"Training Pairs\" (Sequences) ---\n",
        "print(\"\\nCreating training sequences...\")\n",
        "sequence_length = 50 # How many words the model \"sees\" at once to predict the next\n",
        "input_sequences = []\n",
        "\n",
        "# Ensure we have enough text to form at least one sequence\n",
        "if len(numbered_text) < sequence_length + 1:\n",
        "    print(f\"Error: Not enough text to create sequences. Minimum length needed: {sequence_length + 1} tokens.\")\n",
        "    print(f\"Current text length (in tokens): {len(numbered_text)}\")\n",
        "    print(\"Please use a larger text file or reduce 'sequence_length'.\")\n",
        "    # Exit or raise error if data is insufficient for training\n",
        "    exit()\n",
        "\n",
        "for i in range(len(numbered_text) - sequence_length):\n",
        "    # Input: sequence_length words, Output: the (sequence_length + 1)th word\n",
        "    input_seq = numbered_text[i : i + sequence_length]\n",
        "    output_word = numbered_text[i + sequence_length]\n",
        "    input_sequences.append(input_seq + [output_word]) # Store as [word1, ..., word_N, next_word]\n",
        "\n",
        "print(f\"Number of raw sequences created: {len(input_sequences)}\")\n",
        "\n",
        "# Make all sequences the same length (add zeros if needed at the beginning)\n",
        "# The maxlen will be sequence_length + 1 (input sequence + 1 target word)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate into X (input features) and y (what we want to predict)\n",
        "X = padded_sequences[:, :-1] # All columns except the last one (the input sequence)\n",
        "y = padded_sequences[:, -1]  # Only the last column (the word to predict)\n",
        "\n",
        "# Convert 'y' to a special format (one-hot encoding) for the model's output layer\n",
        "# This means for each word, it's represented as a vector of zeros with a '1' at its corresponding index\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"Shape of X (inputs): {X.shape} (Number of sequences, Sequence length)\")\n",
        "print(f\"Shape of y (outputs): {y.shape} (Number of sequences, Vocabulary size)\")\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# You're now ready to define and train the LSTM model in the next cells."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqnnzQyW1H9A",
        "outputId": "04da0582-0d67-4533-d6bc-449a59909a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Successfully loaded text from 'tinyshakespeare.txt'. Length: 5314 characters.\n",
            "\n",
            "Cleaning text...\n",
            "Cleaned text (first 500 chars): first citizen before we proceed any further, hear me speak. all speak, speak. first citizen you are all resolved rather to die than to famish? all resolved. resolved. first citizen first, you know caius marcius is chief enemy to the people. all we knowt, we knowt. first citizen let us kill him, and well have corn at our own price. ist a verdict? all no more talking ont let it be done away, away! second citizen one word, good citizens. first citizen we are accounted poor citizens, the patricians ...\n",
            "Cleaned text length: 5175 characters.\n",
            "\n",
            "Tokenizing words...\n",
            "Total unique words (vocabulary size): 401\n",
            "First 20 words as numbers: [5, 3, 133, 9, 74, 75, 134, 50, 76, 30, 16, 30, 30, 5, 3, 2, 27, 16, 51, 135]\n",
            "\n",
            "Creating training sequences...\n",
            "Number of raw sequences created: 887\n",
            "Shape of X (inputs): (887, 50) (Number of sequences, Sequence length)\n",
            "Shape of y (outputs): (887, 402) (Number of sequences, Vocabulary size)\n",
            "Data preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_ZvurDg1lSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: Model Definition (LSTM)\n",
        "\n",
        "print(\"\\nDefining the LSTM model architecture...\")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "# Embedding Layer: Converts your word numbers into dense vectors that the LSTM can understand better.\n",
        "# total_words is your vocabulary size. 100 is the dimension of the embedding vector.\n",
        "# input_length is the length of your input sequences (X.shape[1]).\n",
        "model.add(Embedding(total_words, 100, input_length=X.shape[1]))\n",
        "\n",
        "# Bidirectional LSTM Layer: Learns patterns from text reading both forwards and backwards.\n",
        "# return_sequences=True means this LSTM layer outputs a sequence for the next LSTM layer.\n",
        "# You can experiment with the number of units (e.g., 150, 200).\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2)) # A common regularization technique to prevent overfitting\n",
        "\n",
        "# Another LSTM Layer: This one processes the output from the first LSTM.\n",
        "# return_sequences=False (default) means it outputs a single vector for the Dense layer.\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax')) # Output layer: predicts the probability for each word in your vocabulary.\n",
        "\n",
        "# Compile the model: This configures the model for training.\n",
        "# 'categorical_crossentropy' is used because your 'y' is one-hot encoded.\n",
        "# 'adam' is a popular optimizer.\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "print(\"Model definition complete!\")\n",
        "\n",
        "# You're now ready to train the LSTM model in the next cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "a8Wc75sq2AGj",
        "outputId": "d71fec63-78f4-44b0-a408-57eaa2230f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining the LSTM model architecture...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definition complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 3: Model Training (LSTM)\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "\n",
        "# Define the number of epochs (how many times the model sees the entire dataset)\n",
        "# For larger datasets, this might take a long time. Start with fewer epochs (e.g., 20)\n",
        "# and increase if your accuracy isn't improving.\n",
        "num_epochs = 50 # You might need to adjust this (e.g., 20, 50, 100, or more)\n",
        "\n",
        "# batch_size: number of samples per gradient update. Adjust based on GPU memory.\n",
        "# Smaller batch sizes are less memory intensive.\n",
        "batch_size = 64 # Common values: 32, 64, 128, 256\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1 # Show progress during training\n",
        ")\n",
        "\n",
        "print(\"\\nModel training finished!\")\n",
        "\n",
        "# Optional: Plot training history to see loss and accuracy over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# You're now ready to generate text in the next cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZydmZ1Ll2IfE",
        "outputId": "c7744713-e3af-44c5-c7c3-6be4eaffc7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node sequential_1/bidirectional_1/forward_lstm_1/CudnnRNNV3 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-4-3431813803.py\", line 14, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 218, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 584, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 402, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 551, in inner_loop\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 933, in _cudnn_lstm\n\nDnn is not supported\n\t [[{{node sequential_1/bidirectional_1/forward_lstm_1/CudnnRNNV3}}]] [Op:__inference_multi_step_on_iterator_4133]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3431813803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;31m# Common values: 32, 64, 128, 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/bidirectional_1/forward_lstm_1/CudnnRNNV3 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-4-3431813803.py\", line 14, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py\", line 218, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 584, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 402, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 551, in inner_loop\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 933, in _cudnn_lstm\n\nDnn is not supported\n\t [[{{node sequential_1/bidirectional_1/forward_lstm_1/CudnnRNNV3}}]] [Op:__inference_multi_step_on_iterator_4133]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: Model Definition (LSTM) - CORRECTED\n",
        "\n",
        "print(\"\\nDefining the LSTM model architecture...\")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "import tensorflow as tf # Ensure tf is imported here if not already\n",
        "\n",
        "# Check if a GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available and will be used.\")\n",
        "else:\n",
        "    print(\"No GPU found. Model will run on CPU, which might be slower.\")\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# Embedding Layer:\n",
        "# total_words: vocabulary size\n",
        "# 100: dimension of the embedding vector\n",
        "# input_length=X.shape[1]: The length of your input sequences (which is `sequence_length`)\n",
        "model.add(Embedding(total_words, 100, input_length=X.shape[1]))\n",
        "\n",
        "# Bidirectional LSTM Layer:\n",
        "# The `input_shape` for the first recurrent layer (like LSTM/GRU/Bidirectional)\n",
        "# after an Embedding layer should be `(input_length, embedding_dimension)`.\n",
        "# However, Keras usually infers this correctly if `input_length` is set in Embedding.\n",
        "# The `InvalidArgumentError` suggests the input to the CuDNN RNN might not be 3D.\n",
        "# Let's ensure the previous layer's output shape is correctly propagated.\n",
        "# The default `return_sequences=False` on the last LSTM layer is fine.\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True))) # 150 is number of LSTM units\n",
        "model.add(Dropout(0.2)) # Randomly turns off some neurons to prevent overfitting\n",
        "\n",
        "model.add(LSTM(100)) # Second LSTM layer\n",
        "model.add(Dense(total_words, activation='softmax')) # Output layer: predicts probability of each word\n",
        "\n",
        "# Compile the model:\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "print(\"Model definition complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "FL2r97Kh2gDV",
        "outputId": "642738d3-6eb2-418a-fcc7-42ee724c37c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining the LSTM model architecture...\n",
            "GPU is available and will be used.\n",
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definition complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Data Loading and Preprocessing\n",
        "\n",
        "# Install necessary libraries if not already present (Colab usually has them)\n",
        "# !pip install tensorflow numpy pandas\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "import os # To check if file exists\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load the Text ---\n",
        "# >>> IMPORTANT: Replace 'your_text_file.txt' with the exact name of your uploaded file. <<<\n",
        "file_name = 'tinyshakespeare.txt' # e.g., 'tinyshakespeare.txt', 'alice.txt'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Error: The file '{file_name}' was not found in the Colab session storage.\")\n",
        "    print(\"Please ensure you have uploaded it using the 'Files' tab on the left sidebar.\")\n",
        "    # You might want to stop execution here if the file is critical\n",
        "    # raise FileNotFoundError(f\"File not found: {file_name}\")\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        raw_text = file.read()\n",
        "    print(f\"Successfully loaded text from '{file_name}'. Length: {len(raw_text)} characters.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # Consider exiting or handling the error if loading fails\n",
        "    exit()\n",
        "\n",
        "# --- 2. Clean It Up ---\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = raw_text.lower() # Convert all text to lowercase\n",
        "\n",
        "# Remove anything that's not a letter, space, or basic punctuation.\n",
        "# Keeping some punctuation can help with coherence for better models.\n",
        "# For simplicity, let's allow letters, spaces, and basic sentence-ending punctuation.\n",
        "cleaned_text = re.sub(r'[^a-z\\s.,!?]', '', cleaned_text)\n",
        "# Replace multiple spaces with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Cleaned text (first 500 chars): {cleaned_text[:500]}...\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)} characters.\")\n",
        "\n",
        "\n",
        "# --- 3. Turn Words into Numbers (Tokenization) ---\n",
        "print(\"\\nTokenizing words...\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text]) # Let the tokenizer learn all unique words\n",
        "\n",
        "word_to_id = tokenizer.word_index # Your word-to-number mapping\n",
        "id_to_word = {v: k for k, v in word_to_id.items()} # Your number-to-word mapping\n",
        "total_words = len(word_to_id) + 1 # Total unique words (+1 for potential OOV or padding)\n",
        "\n",
        "# Convert your entire text into a sequence of numbers\n",
        "numbered_text = tokenizer.texts_to_sequences([cleaned_text])[0]\n",
        "print(f\"Total unique words (vocabulary size): {total_words - 1}\") # -1 because of the +1 for padding token\n",
        "print(f\"First 20 words as numbers: {numbered_text[:20]}\")\n",
        "\n",
        "\n",
        "# --- 4. Create \"Training Pairs\" (Sequences) ---\n",
        "print(\"\\nCreating training sequences...\")\n",
        "sequence_length = 50 # How many words the model \"sees\" at once to predict the next\n",
        "input_sequences = []\n",
        "\n",
        "# Ensure we have enough text to form at least one sequence\n",
        "if len(numbered_text) < sequence_length + 1:\n",
        "    print(f\"Error: Not enough text to create sequences. Minimum length needed: {sequence_length + 1} tokens.\")\n",
        "    print(f\"Current text length (in tokens): {len(numbered_text)}\")\n",
        "    print(\"Please use a larger text file or reduce 'sequence_length'.\")\n",
        "    # Exit or raise error if data is insufficient for training\n",
        "    exit()\n",
        "\n",
        "for i in range(len(numbered_text) - sequence_length):\n",
        "    # Input: sequence_length words, Output: the (sequence_length + 1)th word\n",
        "    input_seq = numbered_text[i : i + sequence_length]\n",
        "    output_word = numbered_text[i + sequence_length]\n",
        "    input_sequences.append(input_seq + [output_word]) # Store as [word1, ..., word_N, next_word]\n",
        "\n",
        "print(f\"Number of raw sequences created: {len(input_sequences)}\")\n",
        "\n",
        "# Make all sequences the same length (add zeros if needed at the beginning)\n",
        "# The maxlen will be sequence_length + 1 (input sequence + 1 target word)\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate into X (input features) and y (what we want to predict)\n",
        "X = padded_sequences[:, :-1] # All columns except the last one (the input sequence)\n",
        "y = padded_sequences[:, -1]  # Only the last column (the word to predict)\n",
        "\n",
        "# Convert 'y' to a special format (one-hot encoding) for the model's output layer\n",
        "# This means for each word, it's represented as a vector of zeros with a '1' at its corresponding index\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"Shape of X (inputs): {X.shape} (Number of sequences, Sequence length)\")\n",
        "print(f\"Shape of y (outputs): {y.shape} (Number of sequences, Vocabulary size)\")\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# You're now ready to define and train the LSTM model in the next cells.\n",
        "# ... (rest of Colab Cell 1 code) ...\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n",
        "\n",
        "# --- ADD THESE TWO LINES FOR DEBUGGING ---\n",
        "print(f\"DEBUG: X.shape before model definition: {X.shape}\")\n",
        "print(f\"DEBUG: X.dtype before model definition: {X.dtype}\")\n",
        "# --- END DEBUG LINES ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi-FnDP-4Aiy",
        "outputId": "8bc48d3c-9be0-421a-a500-d43998bf071d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Successfully loaded text from 'tinyshakespeare.txt'. Length: 5314 characters.\n",
            "\n",
            "Cleaning text...\n",
            "Cleaned text (first 500 chars): first citizen before we proceed any further, hear me speak. all speak, speak. first citizen you are all resolved rather to die than to famish? all resolved. resolved. first citizen first, you know caius marcius is chief enemy to the people. all we knowt, we knowt. first citizen let us kill him, and well have corn at our own price. ist a verdict? all no more talking ont let it be done away, away! second citizen one word, good citizens. first citizen we are accounted poor citizens, the patricians ...\n",
            "Cleaned text length: 5175 characters.\n",
            "\n",
            "Tokenizing words...\n",
            "Total unique words (vocabulary size): 401\n",
            "First 20 words as numbers: [5, 3, 133, 9, 74, 75, 134, 50, 76, 30, 16, 30, 30, 5, 3, 2, 27, 16, 51, 135]\n",
            "\n",
            "Creating training sequences...\n",
            "Number of raw sequences created: 887\n",
            "Shape of X (inputs): (887, 50) (Number of sequences, Sequence length)\n",
            "Shape of y (outputs): (887, 402) (Number of sequences, Vocabulary size)\n",
            "Data preprocessing complete!\n",
            "Data preprocessing complete!\n",
            "DEBUG: X.shape before model definition: (887, 50)\n",
            "DEBUG: X.dtype before model definition: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: Model Definition (LSTM) - REVISED FOR CUDA ERROR AND IMPORTERROR\n",
        "\n",
        "# Import necessary packages for this cell\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional # Input moved here\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\nDefining the LSTM model architecture...\")\n",
        "\n",
        "# Check if a GPU is available and set memory growth (good practice to prevent fragmentation)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is available and memory growth set.\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set at startup\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found. Model will run on CPU, which might be slower.\")\n",
        "\n",
        "model = Sequential()\n",
        "# Input layer explicitly defining the shape expected by the Embedding layer\n",
        "# X.shape[1] is the `sequence_length` which should be defined from Cell 1\n",
        "model.add(Input(shape=(X.shape[1],)))\n",
        "\n",
        "# Embedding Layer:\n",
        "# total_words: vocabulary size (+1 for padding)\n",
        "# 100: dimension of the embedding vector\n",
        "# input_length=X.shape[1]: The length of your input sequences (e.g., 50)\n",
        "model.add(Embedding(input_dim=total_words, output_dim=100, input_length=X.shape[1]))\n",
        "\n",
        "# Bidirectional LSTM Layer:\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Second LSTM Layer\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model:\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "print(\"Model definition complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "rB7QD_Ae56iX",
        "outputId": "ff9ceead-98e4-41cc-e343-6b91cd9f57ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining the LSTM model architecture...\n",
            "GPU is available and memory growth set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m40,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │       \u001b[38;5;34m301,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m160,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m402\u001b[0m)            │        \u001b[38;5;34m40,602\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">301,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">402</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,602</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m542,402\u001b[0m (2.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">542,402</span> (2.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m542,402\u001b[0m (2.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">542,402</span> (2.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definition complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: Model Definition (LSTM) - FORCE NON-CUDNN VERSION FIX\n",
        "\n",
        "# Import necessary packages for this cell\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nDefining the LSTM model architecture (forcing non-CuDNN LSTM)...\")\n",
        "\n",
        "# Check if a GPU is available (optional, but good to know)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is available and memory growth set.\")\n",
        "        print(\"Note: Forcing non-CuDNN LSTM due to previous error. Training might be slightly slower.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found. Model will run on CPU.\")\n",
        "\n",
        "model = Sequential()\n",
        "# Input layer explicitly defining the shape expected by the Embedding layer\n",
        "# X.shape[1] is the `sequence_length` which should be defined from Cell 1\n",
        "model.add(Input(shape=(X.shape[1],)))\n",
        "\n",
        "# Embedding Layer:\n",
        "model.add(Embedding(input_dim=total_words, output_dim=100, input_length=X.shape[1]))\n",
        "\n",
        "# Bidirectional LSTM Layer:\n",
        "# KEY FIX: Setting `recurrent_dropout > 0` (even a small value like 0.1)\n",
        "# forces TensorFlow to use the general (non-CuDNN) LSTM implementation,\n",
        "# which is more robust against these shape/type errors.\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True, recurrent_dropout=0.1))) # ADDED recurrent_dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Second LSTM Layer (also force non-CuDNN)\n",
        "model.add(LSTM(100, recurrent_dropout=0.1)) # ADDED recurrent_dropout\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model:\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "print(\"Model definition complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "8uwlLTxX6iqZ",
        "outputId": "cec03aa4-ce6b-4e62-9520-c34ab0ef8ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining the LSTM model architecture (forcing non-CuDNN LSTM)...\n",
            "GPU is available and memory growth set.\n",
            "Note: Forcing non-CuDNN LSTM due to previous error. Training might be slightly slower.\n",
            "\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m40,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │       \u001b[38;5;34m301,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m160,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m402\u001b[0m)            │        \u001b[38;5;34m40,602\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">301,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">402</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,602</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m542,402\u001b[0m (2.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">542,402</span> (2.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m542,402\u001b[0m (2.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">542,402</span> (2.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model definition complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 3: Model Training (LSTM)\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "\n",
        "# Define the number of epochs (how many times the model sees the entire dataset)\n",
        "# For larger datasets, this might take a long time. Start with fewer epochs (e.g., 20)\n",
        "# and increase if your accuracy isn't improving.\n",
        "num_epochs = 50 # You might need to adjust this (e.g., 20, 50, 100, or more)\n",
        "\n",
        "# batch_size: number of samples per gradient update. Adjust based on GPU memory.\n",
        "# Smaller batch sizes are less memory intensive.\n",
        "batch_size = 64 # Common values: 32, 64, 128, 256\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1 # Show progress during training\n",
        ")\n",
        "\n",
        "print(\"\\nModel training finished!\")\n",
        "\n",
        "# Optional: Plot training history to see loss and accuracy over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# You're now ready to generate text in the next cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KkRUzfSi6t6s",
        "outputId": "7e64115d-30d6-47f9-f6e2-780ec681e89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 476ms/step - accuracy: 0.0323 - loss: 5.9857\n",
            "Epoch 2/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 406ms/step - accuracy: 0.0562 - loss: 5.6497\n",
            "Epoch 3/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 355ms/step - accuracy: 0.0452 - loss: 5.5267\n",
            "Epoch 4/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 476ms/step - accuracy: 0.0581 - loss: 5.4244\n",
            "Epoch 5/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 355ms/step - accuracy: 0.0559 - loss: 5.3917\n",
            "Epoch 6/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 480ms/step - accuracy: 0.0494 - loss: 5.4025\n",
            "Epoch 7/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 483ms/step - accuracy: 0.0680 - loss: 5.3515\n",
            "Epoch 8/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 359ms/step - accuracy: 0.0548 - loss: 5.4546\n",
            "Epoch 9/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 371ms/step - accuracy: 0.0622 - loss: 5.3815\n",
            "Epoch 10/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 365ms/step - accuracy: 0.0599 - loss: 5.3789\n",
            "Epoch 11/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 490ms/step - accuracy: 0.0779 - loss: 5.2902\n",
            "Epoch 12/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 402ms/step - accuracy: 0.0824 - loss: 5.2015\n",
            "Epoch 13/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 423ms/step - accuracy: 0.0796 - loss: 5.1110\n",
            "Epoch 14/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.0827 - loss: 4.9798\n",
            "Epoch 15/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 356ms/step - accuracy: 0.0832 - loss: 4.8738\n",
            "Epoch 16/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 451ms/step - accuracy: 0.0850 - loss: 4.7507\n",
            "Epoch 17/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 356ms/step - accuracy: 0.0785 - loss: 4.7725\n",
            "Epoch 18/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.0958 - loss: 4.7096\n",
            "Epoch 19/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 397ms/step - accuracy: 0.0803 - loss: 4.6613\n",
            "Epoch 20/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 353ms/step - accuracy: 0.0999 - loss: 4.5794\n",
            "Epoch 21/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 476ms/step - accuracy: 0.0949 - loss: 4.5064\n",
            "Epoch 22/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 411ms/step - accuracy: 0.0931 - loss: 4.4932\n",
            "Epoch 23/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 361ms/step - accuracy: 0.0936 - loss: 4.4400\n",
            "Epoch 24/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 470ms/step - accuracy: 0.1170 - loss: 4.3017\n",
            "Epoch 25/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 381ms/step - accuracy: 0.1237 - loss: 4.2908\n",
            "Epoch 26/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 358ms/step - accuracy: 0.1422 - loss: 4.1920\n",
            "Epoch 27/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 476ms/step - accuracy: 0.1363 - loss: 4.2653\n",
            "Epoch 28/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 379ms/step - accuracy: 0.1337 - loss: 4.1783\n",
            "Epoch 29/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 357ms/step - accuracy: 0.1367 - loss: 4.1039\n",
            "Epoch 30/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 478ms/step - accuracy: 0.1255 - loss: 4.1439\n",
            "Epoch 31/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 387ms/step - accuracy: 0.1559 - loss: 4.0195\n",
            "Epoch 32/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 362ms/step - accuracy: 0.1478 - loss: 3.9963\n",
            "Epoch 33/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 480ms/step - accuracy: 0.1736 - loss: 3.9835\n",
            "Epoch 34/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 383ms/step - accuracy: 0.1441 - loss: 3.9667\n",
            "Epoch 35/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 431ms/step - accuracy: 0.1560 - loss: 3.9419\n",
            "Epoch 36/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 482ms/step - accuracy: 0.1617 - loss: 3.8704\n",
            "Epoch 37/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 354ms/step - accuracy: 0.1832 - loss: 3.7980\n",
            "Epoch 38/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 471ms/step - accuracy: 0.1679 - loss: 3.8610\n",
            "Epoch 39/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 456ms/step - accuracy: 0.1670 - loss: 3.8017\n",
            "Epoch 40/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 354ms/step - accuracy: 0.1769 - loss: 3.7829\n",
            "Epoch 41/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.1916 - loss: 3.6597\n",
            "Epoch 42/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 409ms/step - accuracy: 0.2209 - loss: 3.5980\n",
            "Epoch 43/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 397ms/step - accuracy: 0.2120 - loss: 3.6260\n",
            "Epoch 44/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 356ms/step - accuracy: 0.2172 - loss: 3.5524\n",
            "Epoch 45/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 482ms/step - accuracy: 0.2388 - loss: 3.4725\n",
            "Epoch 46/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 435ms/step - accuracy: 0.2148 - loss: 3.5222\n",
            "Epoch 47/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 358ms/step - accuracy: 0.2225 - loss: 3.4453\n",
            "Epoch 48/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 476ms/step - accuracy: 0.2232 - loss: 3.4347\n",
            "Epoch 49/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 355ms/step - accuracy: 0.2516 - loss: 3.3574\n",
            "Epoch 50/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 473ms/step - accuracy: 0.2361 - loss: 3.3874\n",
            "\n",
            "Model training finished!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 4: Text Generation (LSTM)\n",
        "\n",
        "print(\"\\nReady for Text Generation!\")\n",
        "\n",
        "def generate_text(seed_text, next_words_to_generate, model, max_sequence_len, tokenizer, id_to_word, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text using the trained LSTM model.\n",
        "\n",
        "    Args:\n",
        "        seed_text (str): The starting phrase for generation.\n",
        "        next_words_to_generate (int): The number of words to generate.\n",
        "        model (tf.keras.Model): The trained Keras model.\n",
        "        max_sequence_len (int): The fixed length of input sequences for the model.\n",
        "        tokenizer (tf.keras.preprocessing.text.Tokenizer): The tokenizer fitted on the training data.\n",
        "        id_to_word (dict): A dictionary mapping token IDs back to words.\n",
        "        temperature (float): Controls the randomness of prediction.\n",
        "                             Lower (e.g., 0.5) makes text more predictable/focused.\n",
        "                             Higher (e.g., 1.2) makes text more random/creative.\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text.lower() # Start with lowercase seed for consistency\n",
        "\n",
        "    for _ in range(next_words_to_generate):\n",
        "        # Convert the current seed text to numbers\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "\n",
        "        # If the seed text is shorter than the model's expected input, pad it.\n",
        "        # The model expects an input_length of X.shape[1]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "\n",
        "        # Get the model's predictions (probabilities for each possible next word)\n",
        "        # model.predict returns a batch, so take the first element [0]\n",
        "        predicted_probabilities = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply \"temperature\" for creativity (sampling from a distribution)\n",
        "        # This helps to avoid always picking the most probable word, leading to more diverse text.\n",
        "        predicted_probabilities = np.asarray(predicted_probabilities).astype('float64')\n",
        "\n",
        "        # Avoid log(0) for extremely low probabilities by replacing them with a tiny value\n",
        "        predicted_probabilities = np.where(predicted_probabilities == 0, 1e-10, predicted_probabilities)\n",
        "\n",
        "        predicted_probabilities = np.log(predicted_probabilities) / temperature\n",
        "        exp_preds = np.exp(predicted_probabilities)\n",
        "        predicted_probabilities = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Pick the next word based on the (temperature-adjusted) probabilities\n",
        "        # np.random.choice allows sampling based on probabilities\n",
        "        predicted_word_id = np.random.choice(len(predicted_probabilities), p=predicted_probabilities)\n",
        "\n",
        "        output_word = id_to_word.get(predicted_word_id, \"\") # Use .get() to handle potential missing IDs\n",
        "\n",
        "        if output_word: # Only add if a word was found\n",
        "            generated_text += \" \" + output_word\n",
        "        else:\n",
        "            # If no word found for ID (should be rare with good vocabulary), break to prevent infinite loop\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# --- Experiment with different prompts and temperatures ---\n",
        "\n",
        "print(\"\\n--- Generated Text Example 1 ---\")\n",
        "# Choose a starting prompt. Make sure it's long enough to be meaningful to the model (e.g., 5-10 words)\n",
        "# The quality depends heavily on your dataset and training.\n",
        "seed1 = \"the quick brown fox jumps over the\" # Use lowercase for consistency\n",
        "generated1 = generate_text(seed1, 50, model, max_sequence_len, tokenizer, id_to_word, temperature=0.7)\n",
        "print(f\"Prompt: '{seed1}'\")\n",
        "print(f\"Generated: {generated1}\\n\")\n",
        "\n",
        "print(\"\\n--- Generated Text Example 2 ---\")\n",
        "seed2 = \"once upon a time in a faraway land there\"\n",
        "generated2 = generate_text(seed2, 70, model, max_sequence_len, tokenizer, id_to_word, temperature=1.0)\n",
        "print(f\"Prompt: '{seed2}'\")\n",
        "print(f\"Generated: {generated2}\\n\")\n",
        "\n",
        "print(\"\\n--- Generated Text Example 3 (more creative) ---\")\n",
        "seed3 = \"to be or not to be that is the question\" # A classic Shakespeare line!\n",
        "generated3 = generate_text(seed3, 60, model, max_sequence_len, tokenizer, id_to_word, temperature=1.2)\n",
        "print(f\"Prompt: '{seed3}'\")\n",
        "print(f\"Generated: {generated3}\\n\")\n",
        "\n",
        "print(\"Text generation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB78Gll-8Zyg",
        "outputId": "febf19bd-0cd3-49d0-9494-e6ffa5eb4fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ready for Text Generation!\n",
            "\n",
            "--- Generated Text Example 1 ---\n",
            "Prompt: 'the quick brown fox jumps over the'\n",
            "Generated: the quick brown fox jumps over the me even answerd for and virtue to the body as fellow soft disgrace menenius dearth the instruments of the belly that thus menenius like let not that menenius what all the shouts as were thus have good malign you good purpose in need first citizen well cannot the steed and\n",
            "\n",
            "\n",
            "--- Generated Text Example 2 ---\n",
            "Prompt: 'once upon a time in a faraway land there'\n",
            "Generated: once upon a time in a faraway land there either to capitol what the superfluity patricians as more had breaths as you thus make you malign he his their masters be smile may my my citizens you us menenius was not account you think the other charitable an belly eat the commonalty to the who they the way all them have repetition what it menenius and tell you more account first citizen what you ever asunder belly not neer\n",
            "\n",
            "\n",
            "--- Generated Text Example 3 (more creative) ---\n",
            "Prompt: 'to be or not to be that is the question'\n",
            "Generated: to be or not to be that is the question midst bats first citizen can may were in grain bear with if the poor ist other lungs thus edicts as i shall of but first the walk barren sir come he must him with what he senators your maliciously have suffer you a ist ever helps we wondrous honest if in alack the little idle your as your em countrymen\n",
            "\n",
            "Text generation complete!\n"
          ]
        }
      ]
    }
  ]
}